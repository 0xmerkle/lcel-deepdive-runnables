{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohttp==3.9.1 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (3.9.1)\n",
      "Requirement already satisfied: aiosignal==1.3.1 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: annotated-types==0.6.0 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (0.6.0)\n",
      "Requirement already satisfied: anthropic==0.7.7 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (0.7.7)\n",
      "Requirement already satisfied: anyio==3.7.1 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (3.7.1)\n",
      "Requirement already satisfied: attrs==23.1.0 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (23.1.0)\n",
      "Requirement already satisfied: certifi==2023.11.17 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer==3.3.2 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (3.3.2)\n",
      "Requirement already satisfied: dataclasses-json==0.6.3 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (0.6.3)\n",
      "Requirement already satisfied: distro==1.8.0 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (1.8.0)\n",
      "Requirement already satisfied: filelock==3.13.1 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (3.13.1)\n",
      "Requirement already satisfied: frozenlist==1.4.0 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (1.4.0)\n",
      "Requirement already satisfied: fsspec==2023.12.2 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 13)) (2023.12.2)\n",
      "Requirement already satisfied: h11==0.14.0 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 14)) (0.14.0)\n",
      "Requirement already satisfied: httpcore==1.0.2 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 15)) (1.0.2)\n",
      "Requirement already satisfied: httpx==0.25.2 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 16)) (0.25.2)\n",
      "Requirement already satisfied: huggingface-hub==0.19.4 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 17)) (0.19.4)\n",
      "Requirement already satisfied: idna==3.6 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 18)) (3.6)\n",
      "Requirement already satisfied: jsonpatch==1.33 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 19)) (1.33)\n",
      "Requirement already satisfied: jsonpointer==2.4 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 20)) (2.4)\n",
      "Requirement already satisfied: langchain==0.0.348 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 21)) (0.0.348)\n",
      "Requirement already satisfied: langchain-core==0.0.12 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 22)) (0.0.12)\n",
      "Requirement already satisfied: langsmith==0.0.69 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 23)) (0.0.69)\n",
      "Requirement already satisfied: marshmallow==3.20.1 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 24)) (3.20.1)\n",
      "Requirement already satisfied: multidict==6.0.4 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 25)) (6.0.4)\n",
      "Requirement already satisfied: mypy-extensions==1.0.0 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 26)) (1.0.0)\n",
      "Requirement already satisfied: numpy==1.26.2 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 27)) (1.26.2)\n",
      "Requirement already satisfied: openai==1.3.8 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 28)) (1.3.8)\n",
      "Requirement already satisfied: packaging==23.2 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 29)) (23.2)\n",
      "Requirement already satisfied: pydantic==2.5.2 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 30)) (2.5.2)\n",
      "Requirement already satisfied: pydantic_core==2.14.5 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 31)) (2.14.5)\n",
      "Requirement already satisfied: python-dotenv==1.0.0 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 32)) (1.0.0)\n",
      "Requirement already satisfied: PyYAML==6.0.1 in /opt/homebrew/opt/pyyaml/lib/python3.11/site-packages (from -r requirements.txt (line 33)) (6.0.1)\n",
      "Requirement already satisfied: regex==2023.10.3 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 34)) (2023.10.3)\n",
      "Requirement already satisfied: requests==2.31.0 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 35)) (2.31.0)\n",
      "Requirement already satisfied: sniffio==1.3.0 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 36)) (1.3.0)\n",
      "Requirement already satisfied: SQLAlchemy==2.0.23 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 37)) (2.0.23)\n",
      "Requirement already satisfied: tenacity==8.2.3 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 38)) (8.2.3)\n",
      "Requirement already satisfied: tiktoken==0.5.2 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 39)) (0.5.2)\n",
      "Requirement already satisfied: tokenizers==0.15.0 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 40)) (0.15.0)\n",
      "Requirement already satisfied: tqdm==4.66.1 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 41)) (4.66.1)\n",
      "Requirement already satisfied: typing-inspect==0.9.0 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 42)) (0.9.0)\n",
      "Requirement already satisfied: typing_extensions==4.9.0 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 43)) (4.9.0)\n",
      "Requirement already satisfied: urllib3==2.1.0 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 44)) (2.1.0)\n",
      "Requirement already satisfied: yarl==1.9.4 in /opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from -r requirements.txt (line 45)) (1.9.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCEL & Runnables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Notes\n",
    "- Expose schematic information about their input, output and config through their `.input_schema` property, the `.output_schema` property and the `.config_schema` method\n",
    "- LCEL is a declarative way to compose Runnables into chains - any chain constructed with LCEL will also always automatically have sync, async, batch, and streaming support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Primitives\n",
    "#### 1. RunnableSequence\n",
    "- Invokes a series of Runnables sequentially\n",
    "- output of the previous runnable will be input to the next runnable in the chain\n",
    "- can use pipe operator to construct or by passing a list of RunnableSequence\n",
    "- \n",
    "#### 2. Runnable Parallel\n",
    "- Invokes runnables concurrently, providing the same input to each. Construct using dict literal within a sequence or by passing a dict to RunnableParallel\n",
    "\n",
    "### Additional Methods\n",
    "- can be used to modify behavior such as retry policy, lifecycle listeners, make them configurable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging\n",
    "- You can use `set_debug` from `from langchain_core.globals import set_debug`. set_debug(True)\n",
    "- You can pass existing or custom callbacks to any give chain too:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```from langchain_core.tracers import ConsoleCallbackHandler\n",
    "\n",
    "chain.invoke(\n",
    "    ...,\n",
    "    config={'callbacks': [ConsoleCallbackHandler()]}\n",
    ")```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing Data through\n",
    "- RunnablePassthrough allows to pass inputs unchanged or with the addition of extra keys. This typically is used in conjuction with RunnableParallel to assign data to a new key in the map.\n",
    "- RunnablePassthrough() called on itâ€™s own, will simply take the input and pass it through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'passed': {'num': 1}, 'extra': {'num': 1, 'mult': 3}, 'modified': 2}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "runnable = RunnableParallel(\n",
    "    passed=RunnablePassthrough(),\n",
    "    extra=RunnablePassthrough.assign(mult=lambda x: x[\"num\"] * 3),\n",
    "    modified=lambda x: x[\"num\"] + 1,\n",
    ")\n",
    "\n",
    "runnable.invoke({\"num\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `passed` key was called with RunnablePassthrough() and so it passed on {'num': 1}.\n",
    "\n",
    "- In the second line with the key `extra`, we used RunnablePastshrough.assign with a lambda that multiplies the passed integer value by 3. So, `extra` was set with {'num': 1, 'mult': 3} which is the original value with the `'mult'` key added.\n",
    "\n",
    "- Finally, we set a third key called `modified` in the map which uses a labmda to set a single value adding 1 to the num, which resulted in modified key with the value of 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "\n",
    "def length_function(text):\n",
    "    return len(text)\n",
    "\n",
    "\n",
    "def _multiple_length_function(text1, text2):\n",
    "    return len(text1) * len(text2)\n",
    "\n",
    "\n",
    "def multiple_length_function(_dict):\n",
    "    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain1 = prompt | model\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),\n",
    "        \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}\n",
    "        | RunnableLambda(multiple_length_function),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE\n",
    "All inputs to these functions need to be a **SINGLE** argument. If you have a function that accepts multiple arguments, you should write a wrapper that accepts a single input and unpacks it into multiple argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='3 + 9 equals 12.')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We use `itemgetter` to extract the input arguments\n",
    "- `\"a\"` becomes `3` because `itemgetter` gets the value of `\"foo\"` and that is passed through pipe operator to the RunnableLamba which passes the value to the `length_function` which returns length of `\"bar\"` which is `3`\n",
    "- `\"b\"` is 9 because we pass the dictionary of `{\"text1\": \"bar\", \"text2\": \"gah\"}` to the RunnableLambda that passes it into the `multiple_length_function`\n",
    "- the dictionary of `{\"a\":3, \"b\": 9}` is then passed as output to the prompt usuing the pipe operator (`|`)\n",
    "- the output of the prompt again is passed to the model\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accepting Runnable Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'foo': 'bar'}\n",
      "Tokens Used: 65\n",
      "\tPrompt Tokens: 56\n",
      "\tCompletion Tokens: 9\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.00010200000000000001\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableConfig\n",
    "import json\n",
    "\n",
    "\n",
    "def parse_or_fix(text: str, config: RunnableConfig):\n",
    "    fixing_chain = (\n",
    "        ChatPromptTemplate.from_template(\n",
    "            \"Fix the following text:\\n\\n```text\\n{input}\\n```\\nError: {error}\"\n",
    "            \" Don't narrate, just respond with the fixed data.\"\n",
    "        )\n",
    "        | ChatOpenAI()\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            return json.loads(text)\n",
    "        except Exception as e:\n",
    "            text = fixing_chain.invoke({\"input\": text, \"error\": e}, config)\n",
    "    return \"Failed to parse\"\n",
    "\n",
    "\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    output = RunnableLambda(parse_or_fix).invoke(\n",
    "        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}\n",
    "    )\n",
    "    print(output)\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We use RunnableLambda with a custom function that allows us a retry protocol for calling OpenAI models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Routing based on Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Two ways to perform routing:**\n",
    "1. Using a `RunnableBranch`.\n",
    "2. Writing custom factory function that takes the input of a previous step and returns a runnable. Importantly, this should return a runnable and NOT actually execute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnableBranch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"Given the user question below, classify it as either being about `Bodybuilding`, `Jungian Psychology`, or `Other`.\n",
    "\n",
    "        Do not respond with more than one word.\n",
    "\n",
    "        <question>\n",
    "        {question}\n",
    "        </question>\n",
    "\n",
    "        Classification:\"\"\"\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bodybuilding'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"How do I bench press properly?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jungian Psychology'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"How do I do active imagination?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_building_chain =  ( PromptTemplate.from_template(\n",
    "        \"\"\"You are an expert in bodybuilding. \\\n",
    "        Always answer questions starting with \"As Dr. Mike Israetel from Renaissance Periodization told me\". \\\n",
    "        Respond to the following question:\n",
    "        \n",
    "        Question: {question}\n",
    "        Answer:\"\"\"\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    ")\n",
    "jungian_chain = (PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an expert in Jungian Psychology and Theory. \\\n",
    "    Always answer questions starting with \"As Dr. C.G. Jung would say\". \\\n",
    "    Respond to the following question:\n",
    "\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\") | ChatOpenAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableBranch\n",
    "\n",
    "branch = RunnableBranch(\n",
    "    (lambda x: \"bodybuilding\" in x[\"topic\"].lower(), body_building_chain),\n",
    "    (lambda x: \"jungian psychology\" in x[\"topic\"].lower(), jungian_chain),\n",
    "    general_chain,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='As Dr. C.G. Jung would say, to interpret your dreams, it is important to approach them with an open and curious mindset. Dreams, according to Jung, are a window into the unconscious and can provide valuable insights into your psyche. \\n\\nFirstly, record your dreams in a journal immediately upon waking up, as details can easily be forgotten. Pay attention to the emotions, symbols, and themes present in your dreams. \\n\\nNext, recognize that dreams often contain both personal and collective symbols. Personal symbols are unique to each individual and are influenced by personal experiences and memories. Collective symbols, on the other hand, are shared by humanity and can be found across different cultures and time periods. \\n\\nTo interpret your dreams, start by analyzing the personal symbols. Reflect on what these symbols mean to you personally, considering your memories and experiences associated with them. Explore the emotions evoked by these symbols and try to connect them to your waking life circumstances.\\n\\nNext, explore the collective symbols in your dreams. These symbols often tap into archetypal patterns of the collective unconscious, such as the anima/animus, the shadow, or the wise old man/woman. Study the characteristics and meanings associated with these archetypes to gain a deeper understanding of their presence in your dreams.\\n\\nLastly, consider the overall theme and narrative of your dream. Reflect on how it relates to your current life situation, challenges, or desires. Dreams often provide guidance, solutions, or a deeper understanding of unresolved issues.\\n\\nRemember, dream interpretation is a personal and ongoing process. It requires self-reflection, patience, and a willingness to explore the depths of your own psyche. Consulting with a qualified Jungian analyst or therapist can also provide valuable insights and support in understanding your dreams more deeply.')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"how do I interpret my dreams?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The answer to 2 + 2 is 4.')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"whats 2 + 2\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routing with Custom Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route(info):\n",
    "    if \"bodybuilding\" in info[\"topic\"].lower():\n",
    "        return body_building_chain\n",
    "    elif \"jungian psychology\" in info[\"topic\"].lower():\n",
    "        return jungian_chain\n",
    "    else:\n",
    "        return general_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | RunnableLambda(\n",
    "    route\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As Dr. Mike Israetel from Renaissance Periodization told me, to grow your traps, you can focus on incorporating specific exercises that target this muscle group. Some effective exercises include barbell shrugs, dumbbell shrugs, upright rows, and trap bar deadlifts. It's important to ensure proper form and gradually increase the weights as you progress. Additionally, incorporating a variety of rep ranges, including both heavy weights for lower reps and lighter weights for higher reps, can help stimulate muscle growth in the traps. Remember to prioritize progressive overload, prioritize proper nutrition and recovery, and be patient as muscle growth takes time.\")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"how do I grow my traps?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As Dr. C.G. Jung would say, alchemy and dreams are interconnected in the realm of the unconscious. Alchemy, traditionally seen as the precursor to modern chemistry, also held a symbolic significance in psychological terms. Jung considered alchemy to be a symbolic system that mirrored the individuation process, which is the journey towards self-realization and wholeness.\\n\\nIn the context of dreams, alchemy can be seen as a metaphorical language used by the unconscious to convey psychological transformation and inner development. Dreams often contain symbolic elements that parallel alchemical symbols such as the union of opposites, the transformation of base materials into gold, and the concept of the Philosopher's Stone.\\n\\nJust as alchemists worked with various substances and processes to transform lead into gold, dreams can be seen as a psychological laboratory where the unconscious works towards transforming the psyche. Alchemical symbols present in dreams can represent the integration of different aspects of the self or the transformation of unconscious material into conscious awareness.\\n\\nBy studying the alchemical symbols present in dreams, individuals can gain insights into their inner processes and work towards psychological integration. Dreams offer a window into the unconscious, and by engaging with the symbolism present in dreams, individuals can embark on a journey of self-discovery and personal growth.\\n\\nTherefore, alchemy and dreams are connected through their shared language of symbolism and their potential to guide individuals towards psychological transformation and wholeness. As Dr. C.G. Jung would say, exploring the alchemical aspects of dreams can provide valuable insights into the depths of the psyche.\")"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"what does alchemy have to do with dreams?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binding runtime arguments\n",
    "- we can use `Runnable.bind()` to pass arguments as constants so that we can have access to them even within a runnable sequence where the argument is not part of the output of preceding runnables in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: Who was the leader of the Soviet Union during the Cuban Missile Crisis?\n",
      "OPTIONS: \n",
      "a) Nikita Khrushchev\n",
      "b) Vladimir Putin\n",
      "c) Joseph Stalin\n",
      "d) Mikhail Gorbachev\n",
      "SOLUTION: a) Nikita Khrushchev\n",
      "\n",
      "QUESTION: Which country discovered the presence of Soviet missiles in Cuba?\n",
      "OPTIONS: \n",
      "a) United States\n",
      "b) Canada\n",
      "c) Mexico\n",
      "d) Brazil\n",
      "SOLUTION: a) United States\n",
      "\n",
      "QUESTION: How did the United States respond to the discovery of Soviet missiles in Cuba?\n",
      "OPTIONS: \n",
      "a) Imposed a naval blockade\n",
      "b) Launched a military invasion\n",
      "c) Sent a diplomatic envoy\n",
      "d) Ignored the situation\n",
      "SOLUTION: a) Imposed a naval blockade\n",
      "\n",
      "QUESTION: How did the Cuban Missile Crisis end?\n",
      "OPTIONS: \n",
      "a) The United States and Soviet Union engaged in a nuclear war\n",
      "b) The United States and Soviet Union reached a peaceful resolution\n",
      "c) Cuba launched a missile attack on the United States\n",
      "d) The United Nations intervened and resolved the crisis\n",
      "SOLUTION: b) The United States and Soviet Union reached a peaceful resolution\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Write out 4 flashcard questions with their options based on the topic given below.. Use the format\\n\\QUESTIONS:...\\nOPTIONS:...\\nSOLUTION:...\\n\\n\",\n",
    "        ),\n",
    "        (\"human\", \"{topic}\"),\n",
    "    ]\n",
    ")\n",
    "model = ChatOpenAI(temperature=0)\n",
    "runnable = (\n",
    "    {\"topic\": RunnablePassthrough()} | prompt | model | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(runnable.invoke(\"the cuban missile crisis\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: Who was the leader of the Soviet Union during the Cuban Missile Crisis?\n",
      "OPTIONS: \n",
      "a) Nikita Khrushchev\n",
      "b) Vladimir Putin\n",
      "c) Joseph Stalin\n",
      "d) Mikhail Gorbachev\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runnable = (\n",
    "    {\"topic\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model.bind(stop=\"SOLUTION\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(runnable.invoke(\"the cuban missile crisis\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach OpenAI Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "function = {\n",
    "    \"name\": \"return_questions\",\n",
    "    \"description\": \"extracts questions from raw text\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"raw_text\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The raw text of flashcard questions\",\n",
    "            },\n",
    "            \"questions\": {\n",
    "                \"type\": \"array\",\n",
    "                \"description\": \"array of questions\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"question string\"\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"equation\", \"solution\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"questions\": [\\n    \"What event sparked the beginning of World War 2?\",\\n    \"Who were the Axis Powers in World War 2?\",\\n    \"Who were the Allied Powers in World War 2?\",\\n    \"What was the Holocaust?\",\\n    \"What was the significance of the Battle of Stalingrad?\",\\n    \"What was the role of the United States in World War 2?\",\\n    \"What was the Manhattan Project?\",\\n    \"What was the significance of D-Day?\",\\n    \"What were the Nuremberg Trials?\",\\n    \"What was the outcome of World War 2?\",\\n    \"Who was the leader of Nazi Germany during World War 2?\",\\n    \"What was the role of Japan in World War 2?\",\\n    \"What was the impact of World War 2 on the world?\",\\n    \"What was the role of women during World War 2?\",\\n    \"What was the Blitz?\",\\n    \"What was the significance of the Battle of Midway?\",\\n    \"What was the Atlantic Charter?\",\\n    \"What was the Yalta Conference?\",\\n    \"What was the Potsdam Conference?\",\\n    \"What was the role of the Soviet Union in World War 2?\"\\n  ]\\n}', 'name': 'return_questions'}})"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need gpt-4 to solve this one correctly\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Write out flashcard questions to the following topic then return the list of questions.\",\n",
    "        ),\n",
    "        (\"human\", \"{topic}\"),\n",
    "    ]\n",
    ")\n",
    "model = ChatOpenAI(model=\"gpt-4\", temperature=0).bind(\n",
    "    function_call={\"name\": \"return_questions\"}, functions=[function]\n",
    ")\n",
    "runnable = {\"topic\": RunnablePassthrough()} | prompt | model\n",
    "runnable.invoke(\"world war 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fallbacks\n",
    "- we can use fallbacks at the runnable level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatAnthropic, ChatOpenAI\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"ANTHROPIC_API_KEY\"] = os.getenv(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import patch\n",
    "\n",
    "import httpx\n",
    "from openai import RateLimitError\n",
    "\n",
    "request = httpx.Request(\"GET\", \"/\")\n",
    "response = httpx.Response(200, request=request)\n",
    "error = RateLimitError(\"rate limit\", response=response, body=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we set max_retries = 0 to avoid retrying on RateLimits, etc\n",
    "openai_llm = ChatOpenAI(max_retries=0)\n",
    "anthropic_llm = ChatAnthropic()\n",
    "llm = openai_llm.with_fallbacks([anthropic_llm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit error\n"
     ]
    }
   ],
   "source": [
    "# Let's use just the OpenAI LLm first, to show that we run into an error\n",
    "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
    "    try:\n",
    "        print(openai_llm.invoke(\"Why did the chicken cross the road?\"))\n",
    "    except RateLimitError:\n",
    "        print(\"Hit error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=' I don\\'t have enough context to determine the chicken\\'s true motivation, but the classic punchline is: \"To get to the other side!\" It\\'s an anti-joke playing on the double meaning of \"the other side\" referring to either the other side of the road, or the afterlife. Without more details, I\\'d just be clucking in the dark trying to explain this chicken\\'s reasoning.'\n"
     ]
    }
   ],
   "source": [
    "# Now let's try with fallbacks to Anthropic\n",
    "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
    "    try:\n",
    "        print(llm.invoke(\"Why did the chicken cross the road?\"))\n",
    "    except RateLimitError:\n",
    "        print(\"Hit error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\" I don't know, why did the kangaroo cross the road?\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You're a nice assistant who always includes a compliment in your response\",\n",
    "        ),\n",
    "        (\"human\", \"Why did the {animal} cross the road\"),\n",
    "    ]\n",
    ")\n",
    "chain = prompt | llm\n",
    "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
    "    try:\n",
    "        print(chain.invoke({\"animal\": \"kangaroo\"}))\n",
    "    except RateLimitError:\n",
    "        print(\"Hit error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's create a chain with a ChatModel\n",
    "# We add in a string output parser here so the outputs between the two are the same type\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You're a nice assistant who always includes a compliment in your response\",\n",
    "        ),\n",
    "        (\"human\", \"Why did the {animal} cross the road\"),\n",
    "    ]\n",
    ")\n",
    "# Here we're going to use a bad model name to easily create a chain that will error\n",
    "chat_model = ChatOpenAI(model_name=\"gpt-fake\")\n",
    "bad_chain = chat_prompt | chat_model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets create a chain with the normal OpenAI model\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Instructions: You should always include a compliment in your response.\n",
    "\n",
    "Question: Why did the {animal} cross the road?\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "llm = OpenAI()\n",
    "good_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nAnswer: The turtle crossed the road to get to the other side! That's a great question, by the way - you have a great sense of humor!\""
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now create a final chain which combines the two\n",
    "chain = bad_chain.with_fallbacks([good_chain])\n",
    "chain.invoke({\"animal\": \"turtle\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
